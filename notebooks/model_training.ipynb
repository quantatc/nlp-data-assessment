{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fcfabc",
   "metadata": {},
   "source": [
    "## Model Training – ML & Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15839366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Deep Learning imports (TensorFlow/Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, concatenate, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4fd64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19999 entries, 0 to 19998\n",
      "Data columns (total 24 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   patient_id       19999 non-null  int64  \n",
      " 1   gender           19999 non-null  object \n",
      " 2   medication       19999 non-null  object \n",
      " 3   dose             16651 non-null  object \n",
      " 4   name             19999 non-null  object \n",
      " 5   surname          19999 non-null  object \n",
      " 6   bmi              19999 non-null  float64\n",
      " 7   weight           19999 non-null  float64\n",
      " 8   height           19999 non-null  float64\n",
      " 9   systolic         19999 non-null  int64  \n",
      " 10  diastolic        19999 non-null  int64  \n",
      " 11  concentration    19999 non-null  int64  \n",
      " 12  distractibility  19999 non-null  int64  \n",
      " 13  impulsivity      19999 non-null  int64  \n",
      " 14  hyperactivity    19999 non-null  int64  \n",
      " 15  sleep            19999 non-null  int64  \n",
      " 16  mood             19999 non-null  int64  \n",
      " 17  appetite         19999 non-null  int64  \n",
      " 18  doctor_notes     19999 non-null  object \n",
      " 19  is_medicated     19999 non-null  int64  \n",
      " 20  dose_mg          19999 non-null  float64\n",
      " 21  bmi_category     19999 non-null  object \n",
      " 22  bp_category      19999 non-null  object \n",
      " 23  processed_notes  19999 non-null  object \n",
      "dtypes: float64(4), int64(11), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#load processed data\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/processed/processed_patient_behavior_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58046df6",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81909d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing features for predicting 'concentration'...\n",
      "Shifting target 'concentration' by adding 2 and converting to int for classification.\n",
      "  Target 'concentration' unique values after adjustment: [0 1 2 3 4]\n",
      "  Features for tabular model: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg', 'gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified categorical features: ['gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified numerical features: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg']\n",
      "Target 'concentration' (y_rf) unique values for RF model: [0 1 2 3 4]\n",
      "Target 'concentration' (y_rf) dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features: encode categorical and normalise numeric\n",
    "def prepare_features_for_tabular_model(df_input, target_column):\n",
    "    \"\"\"\n",
    "    Prepares tabular features for ML models.\n",
    "    Separates features and target, encodes categorical, scales numerical.\n",
    "    Adjusts target for classification if necessary.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing features for predicting '{target_column}'...\")\n",
    "    df = df_input.copy()\n",
    "\n",
    "    if target_column not in df.columns:\n",
    "        print(f\"Error: Target column '{target_column}' not found in DataFrame.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    y = df[target_column] # Define y first\n",
    "\n",
    "    # Adjust target for classification: shift and convert to int\n",
    "    # Original range is -2 to 2, shifting by 2 to get 0 to 4\n",
    "    if y.min() < 0: # Basic check if shifting is needed\n",
    "        print(f\"Shifting target '{target_column}' by adding 2 and converting to int for classification.\")\n",
    "        y = (y + 2).astype(int)\n",
    "    else: # If already non-negative, ensure it's int\n",
    "        y = y.astype(int)\n",
    "    print(f\"  Target '{target_column}' unique values after adjustment: {np.sort(y.unique())}\")\n",
    "\n",
    "\n",
    "    # Define features (X)\n",
    "    behavioral_targets = ['concentration', 'impulsivity', 'mood', 'sleep', 'appetite', 'distractibility', 'hyperactivity']\n",
    "    potential_other_targets = [col for col in behavioral_targets if col != target_column]\n",
    "    \n",
    "    columns_to_drop_for_X = ['patient_id', 'name', 'surname', \n",
    "                                   'doctor_notes', 'dose', \n",
    "                                   'processed_notes', 'processed_notes_nltk', \n",
    "                                   target_column\n",
    "                                  ] + potential_other_targets\n",
    "    \n",
    "    cols_to_drop_existing = [col for col in columns_to_drop_for_X if col in df.columns]\n",
    "    X_tabular = df.drop(columns=cols_to_drop_existing, errors='ignore')\n",
    "    \n",
    "    categorical_features = X_tabular.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = X_tabular.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    X_tabular = X_tabular[numerical_features + categorical_features].copy() \n",
    "\n",
    "    print(f\"  Features for tabular model: {X_tabular.columns.tolist()}\")\n",
    "    print(f\"  Identified categorical features: {categorical_features}\")\n",
    "    print(f\"  Identified numerical features: {numerical_features}\")\n",
    "    \n",
    "    if X_tabular.empty and not (numerical_features or categorical_features) :\n",
    "        print(\"Error: Feature set X_tabular is empty after dropping columns and identifying types.\")\n",
    "        return None, None, None\n",
    "    elif not numerical_features and not categorical_features and not X_tabular.empty :\n",
    "         print(\"Warning: No specific numerical or categorical features identified by dtype, but X_tabular is not empty.\")\n",
    "    \n",
    "    transformers_list = []\n",
    "    if numerical_features:\n",
    "        numerical_transformer = StandardScaler()\n",
    "        transformers_list.append(('num', numerical_transformer, numerical_features))\n",
    "    if categorical_features:\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False) \n",
    "        transformers_list.append(('cat', categorical_transformer, categorical_features))\n",
    "\n",
    "    if not transformers_list and not X_tabular.empty:\n",
    "        print(\"Warning: No transformers created. Preprocessor will use remainder='passthrough'.\")\n",
    "        preprocessor = ColumnTransformer(transformers=[], remainder='passthrough') \n",
    "    elif not X_tabular.empty:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=transformers_list, \n",
    "            remainder='passthrough' \n",
    "        )\n",
    "    else: \n",
    "        preprocessor = None\n",
    "        print(\"Error: Cannot create preprocessor as X_tabular is empty.\")\n",
    "    \n",
    "    return X_tabular, y, preprocessor\n",
    "\n",
    "X_tab_rf, y_rf, preprocessor_rf = prepare_features_for_tabular_model(df, target_column='concentration')\n",
    "\n",
    "# Display y_rf's new range to confirm\n",
    "if y_rf is not None:\n",
    "    print(f\"Target 'concentration' (y_rf) unique values for RF model: {np.sort(y_rf.unique())}\")\n",
    "    print(f\"Target 'concentration' (y_rf) dtype: {y_rf.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "144a5130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>is_medicated</th>\n",
       "      <th>dose_mg</th>\n",
       "      <th>gender</th>\n",
       "      <th>medication</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>bp_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.8</td>\n",
       "      <td>82.9</td>\n",
       "      <td>1.76</td>\n",
       "      <td>113</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Adderall</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.3</td>\n",
       "      <td>52.3</td>\n",
       "      <td>1.74</td>\n",
       "      <td>136</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>NoMedication</td>\n",
       "      <td>Underweight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.5</td>\n",
       "      <td>114.9</td>\n",
       "      <td>1.80</td>\n",
       "      <td>128</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Vyvanse</td>\n",
       "      <td>Obese</td>\n",
       "      <td>Elevated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bmi  weight  height  systolic  diastolic  is_medicated  dose_mg gender  \\\n",
       "0  26.8    82.9    1.76       113         88             1     15.0  Other   \n",
       "1  17.3    52.3    1.74       136         72             0      0.0  Other   \n",
       "2  35.5   114.9    1.80       128         77             1     15.0   Male   \n",
       "\n",
       "     medication bmi_category           bp_category  \n",
       "0      Adderall   Overweight  Hypertension Stage 1  \n",
       "1  NoMedication  Underweight  Hypertension Stage 1  \n",
       "2       Vyvanse        Obese              Elevated  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab_rf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aefc1c",
   "metadata": {},
   "source": [
    "### 3.2 2.\tBaseline: Predict `concentration` using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179cef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_rf_class for RF training: [0 1 2 3 4]\n",
      "\n",
      "--- Training Random Forest Baseline to predict 'concentration' (Classification) ---\n",
      "Training Random Forest Classifier...\n",
      "\n",
      "Random Forest Classifier Evaluation for 'concentration':\n",
      "  Accuracy: 0.2003\n",
      "  F1 Score (weighted): 0.2002\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.20      0.20       799\n",
      "           1       0.20      0.20      0.20       803\n",
      "           2       0.20      0.20      0.20       800\n",
      "           3       0.23      0.22      0.22       792\n",
      "           4       0.19      0.17      0.18       806\n",
      "\n",
      "    accuracy                           0.20      4000\n",
      "   macro avg       0.20      0.20      0.20      4000\n",
      "weighted avg       0.20      0.20      0.20      4000\n",
      "\n",
      "\n",
      "Top 10 Feature Importances (Random Forest Classifier):\n",
      "                 feature  importance\n",
      "1                 weight    0.158980\n",
      "0                    bmi    0.155571\n",
      "2                 height    0.143152\n",
      "3               systolic    0.139091\n",
      "4              diastolic    0.128842\n",
      "6                dose_mg    0.076629\n",
      "8            gender_Male    0.016543\n",
      "9           gender_Other    0.016116\n",
      "13    medication_Ritalin    0.014524\n",
      "14  medication_Strattera    0.014115\n"
     ]
    }
   ],
   "source": [
    "def train_random_forest_baseline(X_tabular_input, y_target_input, preprocessor_input):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest Classifier to predict the target.\n",
    "    \"\"\"\n",
    "    target_name = y_target_input.name\n",
    "    print(f\"\\n--- Training Random Forest Baseline to predict '{target_name}' (Classification) ---\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tabular_input, y_target_input, test_size=0.2, random_state=42, stratify=y_target_input if y_target_input.nunique() > 1 else None) # Added stratify\n",
    "\n",
    "    # Create the full pipeline including preprocessing and the model\n",
    "    # RandomForestClassifier for classification\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced') # Added class_weight\n",
    "    \n",
    "    pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor_input),\n",
    "                                  ('classifier', rf_model)]) # Changed regressor to classifier\n",
    "\n",
    "    print(\"Training Random Forest Classifier...\")\n",
    "    pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_rf = pipeline_rf.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nRandom Forest Classifier Evaluation for '{target_name}':\")\n",
    "    \n",
    "    # Classification metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    # For multi-class, specify average for F1 score, e.g., 'weighted' or 'macro'\n",
    "    # Since target was -2 to 2, now 0 to 4, it's multi-class\n",
    "    f1 = f1_score(y_test, y_pred_rf, average='weighted') \n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1 Score (weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Ensure y_test and y_pred_rf have the same labels present or specify labels in classification_report\n",
    "    # target_names can be created if you have a mapping from 0-4 back to original labels\n",
    "    # For now, using default numeric labels\n",
    "    print(classification_report(y_test, y_pred_rf, zero_division=0)) # Added zero_division\n",
    "\n",
    "    # Feature importances\n",
    "    try:\n",
    "        # Get feature names after one-hot encoding\n",
    "        cat_features_in_X_train = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Check if 'cat' transformer exists and has features\n",
    "        if 'cat' in pipeline_rf.named_steps['preprocessor'].named_transformers_ and len(cat_features_in_X_train) > 0 :\n",
    "            ohe_feature_names = pipeline_rf.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(\n",
    "                cat_features_in_X_train\n",
    "            )\n",
    "        else:\n",
    "            ohe_feature_names = np.array([]) # Empty array if no categorical features or no 'cat' transformer\n",
    "\n",
    "        num_feature_names = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Handle case where passthrough columns might exist and are not numbers (should not happen with current X_tabular selection)\n",
    "        # For simplicity, assuming all_feature_names can be constructed this way\n",
    "        all_feature_names = np.concatenate([num_feature_names, ohe_feature_names])\n",
    "        \n",
    "        importances = pipeline_rf.named_steps['classifier'].feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "        print(\"\\nTop 10 Feature Importances (Random Forest Classifier):\")\n",
    "        print(feature_importance_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve feature importances: {e}\")\n",
    "        \n",
    "    return pipeline_rf # Return the trained pipeline\n",
    "\n",
    "# Assuming X_tab_rf and y_rf are already prepared with y_rf as integer classes\n",
    "if X_tab_rf is not None and y_rf is not None and preprocessor_rf is not None:\n",
    "    # Ensure y_rf is int for classification\n",
    "    if not pd.api.types.is_integer_dtype(y_rf):\n",
    "        print(\"Warning: y_rf is not integer type. Attempting conversion (this should have been handled in prepare_features).\")\n",
    "        # This is a safeguard; prepare_features_for_tabular_model should already handle it.\n",
    "        y_rf_class = (y_rf + (2 if y_rf.min() < 0 else 0)).astype(int)\n",
    "    else:\n",
    "        y_rf_class = y_rf\n",
    "\n",
    "    print(f\"Unique values in y_rf_class for RF training: {np.sort(y_rf_class.unique())}\")\n",
    "    rf_pipeline_trained = train_random_forest_baseline(X_tab_rf, y_rf_class, preprocessor_rf)\n",
    "else:\n",
    "    print(\"Skipping Random Forest training as inputs are not ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732cddc9",
   "metadata": {},
   "source": [
    "### 3.3.1\tDeep models: Predict `impulsivity` from doctor_notes using LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a700e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LSTM Model to predict 'impulsivity' (Classification) ---\n",
      "Shifting target 'impulsivity' for LSTM by adding 2 and converting to int.\n",
      "  Target 'impulsivity' for LSTM - Unique classes: [0 1 2 3 4], Num classes: 5\n",
      "  Found 822 unique tokens.\n",
      "  Shape of padded sequences: (19999, 150)\n",
      "  Building LSTM model architecture for classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AlgorithmicTradingProjects\\nlp-data-assessment\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │        \u001b[38;5;34m82,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m42,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,785</span> (495.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,785\u001b[0m (495.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,785</span> (495.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m126,785\u001b[0m (495.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training LSTM model...\n",
      "Epoch 1/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.2139 - loss: 1.6101 - val_accuracy: 0.2113 - val_loss: 1.6090\n",
      "Epoch 2/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 103ms/step - accuracy: 0.1945 - loss: 1.6100 - val_accuracy: 0.2113 - val_loss: 1.6091\n",
      "Epoch 3/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 102ms/step - accuracy: 0.2025 - loss: 1.6095 - val_accuracy: 0.2013 - val_loss: 1.6091\n",
      "Epoch 4/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 104ms/step - accuracy: 0.2034 - loss: 1.6095 - val_accuracy: 0.2013 - val_loss: 1.6092\n",
      "Epoch 5/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 105ms/step - accuracy: 0.2027 - loss: 1.6096 - val_accuracy: 0.2113 - val_loss: 1.6089\n",
      "Epoch 6/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 106ms/step - accuracy: 0.1988 - loss: 1.6098 - val_accuracy: 0.2113 - val_loss: 1.6087\n",
      "Epoch 7/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 110ms/step - accuracy: 0.1991 - loss: 1.6097 - val_accuracy: 0.2013 - val_loss: 1.6090\n",
      "Epoch 8/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 105ms/step - accuracy: 0.2016 - loss: 1.6094 - val_accuracy: 0.2113 - val_loss: 1.6091\n",
      "Epoch 9/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 107ms/step - accuracy: 0.2077 - loss: 1.6095 - val_accuracy: 0.2013 - val_loss: 1.6091\n",
      "Epoch 10/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 106ms/step - accuracy: 0.1988 - loss: 1.6096 - val_accuracy: 0.2013 - val_loss: 1.6091\n",
      "Epoch 11/30\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 105ms/step - accuracy: 0.2086 - loss: 1.6093 - val_accuracy: 0.2013 - val_loss: 1.6091\n",
      "\n",
      "  LSTM Model Evaluation on Test Set (Classification):\n",
      "  Test Loss: 1.6094\n",
      "  Test Accuracy: 0.2042\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step\n",
      "  Test F1 Score (weighted): 0.0693\n",
      "\n",
      "  Classification Report (LSTM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       795\n",
      "           1       0.00      0.00      0.00       793\n",
      "           2       0.00      0.00      0.00       782\n",
      "           3       0.00      0.00      0.00       813\n",
      "           4       0.20      1.00      0.34       817\n",
      "\n",
      "    accuracy                           0.20      4000\n",
      "   macro avg       0.04      0.20      0.07      4000\n",
      "weighted avg       0.04      0.20      0.07      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Deep models: LSTM : Predict impulsivity from doctor_notes ---\n",
    "MAX_VOCAB_SIZE_LSTM = 10000\n",
    "MAX_SEQUENCE_LENGTH_LSTM = 150 # Max length of sequences (notes)\n",
    "EMBEDDING_DIM_LSTM = 100 # Dimension of word embeddings\n",
    "\n",
    "def train_lstm_model(df_input, target_column='impulsivity'):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model to predict a target (e.g., 'impulsivity') from 'processed_notes'\n",
    "    as a classification task.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training LSTM Model to predict '{target_column}' (Classification) ---\")\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    if target_column not in df.columns or 'processed_notes' not in df.columns:\n",
    "        print(f\"Error: Missing '{target_column}' or 'processed_notes' in DataFrame for LSTM.\")\n",
    "        return None, None, None\n",
    "\n",
    "    texts = df['processed_notes'].astype(str).values # Ensure string type\n",
    "    labels_original = df[target_column].values\n",
    "\n",
    "    # Adjust labels for classification: shift if negative, then convert to int\n",
    "    if labels_original.min() < 0:\n",
    "        print(f\"Shifting target '{target_column}' for LSTM by adding 2 and converting to int.\")\n",
    "        labels_adjusted = (labels_original + 2).astype(int)\n",
    "    else:\n",
    "        labels_adjusted = labels_original.astype(int)\n",
    "    \n",
    "    num_classes = len(np.unique(labels_adjusted))\n",
    "    print(f\"  Target '{target_column}' for LSTM - Unique classes: {np.sort(np.unique(labels_adjusted))}, Num classes: {num_classes}\")\n",
    "\n",
    "\n",
    "    # Tokenizer: Convert text to sequences of integers\n",
    "    tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE_LSTM, oov_token=\"<oov>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(f\"  Found {len(word_index)} unique tokens.\")\n",
    "\n",
    "    # Pad sequences to ensure uniform length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH_LSTM, padding='post', truncating='post')\n",
    "    print(f\"  Shape of padded sequences: {padded_sequences.shape}\")\n",
    "\n",
    "    y_lstm = labels_adjusted # Use the adjusted integer labels\n",
    "\n",
    "    # Split data\n",
    "    X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "        padded_sequences, y_lstm, test_size=0.2, random_state=42, stratify=y_lstm if num_classes > 1 else None\n",
    "    )\n",
    "\n",
    "    # Build LSTM model\n",
    "    print(\"  Building LSTM model architecture for classification...\")\n",
    "    input_lstm = Input(shape=(MAX_SEQUENCE_LENGTH_LSTM,))\n",
    "    embedding_layer = Embedding(input_dim=min(MAX_VOCAB_SIZE_LSTM, len(word_index) + 1), \n",
    "                                output_dim=EMBEDDING_DIM_LSTM, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH_LSTM)(input_lstm)\n",
    "    lstm_layer = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embedding_layer) \n",
    "    dense_layer = Dense(32, activation='relu')(lstm_layer)\n",
    "    dropout_layer = Dropout(0.3)(dense_layer)\n",
    "    # Output layer for multi-class classification\n",
    "    output_layer = Dense(num_classes, activation='softmax')(dropout_layer) \n",
    "\n",
    "    model_lstm = Model(inputs=input_lstm, outputs=output_layer)\n",
    "    # Compile for classification\n",
    "    model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_lstm.summary()\n",
    "\n",
    "    print(\"\\n  Training LSTM model...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history_lstm = model_lstm.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=30, \n",
    "        batch_size=32,\n",
    "        validation_split=0.1, \n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate LSTM model\n",
    "    print(\"\\n  LSTM Model Evaluation on Test Set (Classification):\")\n",
    "    loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "    print(f\"  Test Loss: {loss_lstm:.4f}\")\n",
    "    print(f\"  Test Accuracy: {accuracy_lstm:.4f}\")\n",
    "\n",
    "    y_pred_probs_lstm = model_lstm.predict(X_test_lstm)\n",
    "    y_pred_lstm = np.argmax(y_pred_probs_lstm, axis=1)\n",
    "    \n",
    "    f1_lstm = f1_score(y_test_lstm, y_pred_lstm, average='weighted')\n",
    "    print(f\"  Test F1 Score (weighted): {f1_lstm:.4f}\")\n",
    "    print(\"\\n  Classification Report (LSTM):\")\n",
    "    print(classification_report(y_test_lstm, y_pred_lstm, zero_division=0))\n",
    "    \n",
    "    return model_lstm, tokenizer, history_lstm\n",
    "\n",
    "# Run LSTM Model (Predicting impulsivity from doctor_notes)\n",
    "# Ensure 'impulsivity' and 'processed_notes' exist, and drop NaNs\n",
    "df_for_lstm_prep = df[['processed_notes', 'impulsivity']].copy().dropna()\n",
    "\n",
    "if not df_for_lstm_prep.empty:\n",
    "    lstm_model_trained, lstm_tokenizer, lstm_history = train_lstm_model(df_for_lstm_prep, target_column='impulsivity')\n",
    "else:\n",
    "    print(\"DataFrame for LSTM is empty after dropping NaNs. Skipping LSTM training.\")\n",
    "    lstm_model_trained, lstm_tokenizer, lstm_history = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e130978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634dc5bd",
   "metadata": {},
   "source": [
    "### 3.3.2\tDeep models: Predict `concentration` from doctor_notes using CLSTM (CNN + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d46f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training CLSTM Model to predict 'concentration' (Classification) ---\n",
      "\n",
      "Preparing features for predicting 'concentration'...\n",
      "Shifting target 'concentration' by adding 2 and converting to int for classification.\n",
      "  Target 'concentration' unique values after adjustment: [0 1 2 3 4]\n",
      "  Features for tabular model: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg', 'gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified categorical features: ['gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified numerical features: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg']\n",
      "  Target 'concentration' for CLSTM - Unique classes: [0 1 2 3 4], Num classes: 5\n",
      "  Using provided tokenizer for CLSTM text data...\n",
      "  Found 822 unique tokens for CLSTM (using provided tokenizer).\n",
      "  Shape of padded text sequences for CLSTM: (19999, 150)\n",
      "  Preprocessing CLSTM tabular data (fitting on train split)...\n",
      "  CLSTM Train shapes: Tabular (15999, 24), Text (15999, 150), Target (15999,)\n",
      "  CLSTM Test shapes: Tabular (4000, 24), Text (4000, 150), Target (4000,)\n",
      "  Building CLSTM model architecture for classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AlgorithmicTradingProjects\\nlp-data-assessment\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tabular_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,300</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │ tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tabular_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │     \u001b[38;5;34m82,300\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,600\u001b[0m │ tabular_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m42,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,785</span> (526.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m134,785\u001b[0m (526.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,785</span> (526.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m134,785\u001b[0m (526.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training CLSTM model...\n",
      "Epoch 1/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 112ms/step - accuracy: 0.2022 - loss: 1.6197 - val_accuracy: 0.2094 - val_loss: 1.6095\n",
      "Epoch 2/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 133ms/step - accuracy: 0.2058 - loss: 1.6105 - val_accuracy: 0.2025 - val_loss: 1.6095\n",
      "Epoch 3/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 134ms/step - accuracy: 0.2064 - loss: 1.6090 - val_accuracy: 0.1950 - val_loss: 1.6109\n",
      "Epoch 4/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 128ms/step - accuracy: 0.2104 - loss: 1.6083 - val_accuracy: 0.1963 - val_loss: 1.6107\n",
      "Epoch 5/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 136ms/step - accuracy: 0.2087 - loss: 1.6086 - val_accuracy: 0.2181 - val_loss: 1.6090\n",
      "Epoch 6/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 138ms/step - accuracy: 0.2151 - loss: 1.6076 - val_accuracy: 0.1919 - val_loss: 1.6098\n",
      "Epoch 7/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 137ms/step - accuracy: 0.2230 - loss: 1.6059 - val_accuracy: 0.2000 - val_loss: 1.6109\n",
      "Epoch 8/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 135ms/step - accuracy: 0.2201 - loss: 1.6046 - val_accuracy: 0.2056 - val_loss: 1.6096\n",
      "Epoch 9/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 131ms/step - accuracy: 0.2247 - loss: 1.6047 - val_accuracy: 0.2106 - val_loss: 1.6117\n",
      "Epoch 10/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 128ms/step - accuracy: 0.2326 - loss: 1.6031 - val_accuracy: 0.2050 - val_loss: 1.6123\n",
      "Epoch 11/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 138ms/step - accuracy: 0.2229 - loss: 1.6034 - val_accuracy: 0.1863 - val_loss: 1.6124\n",
      "Epoch 12/40\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 138ms/step - accuracy: 0.2239 - loss: 1.6024 - val_accuracy: 0.2006 - val_loss: 1.6131\n",
      "\n",
      "  CLSTM Model Evaluation on Test Set (Classification):\n",
      "  Test Loss: 1.6098\n",
      "  Test Accuracy: 0.1992\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step\n",
      "  Test F1 Score (weighted): 0.1712\n",
      "\n",
      "  Classification Report (CLSTM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.09      0.13       799\n",
      "           1       0.19      0.25      0.22       803\n",
      "           2       0.21      0.28      0.24       800\n",
      "           3       0.16      0.01      0.02       792\n",
      "           4       0.19      0.37      0.25       806\n",
      "\n",
      "    accuracy                           0.20      4000\n",
      "   macro avg       0.20      0.20      0.17      4000\n",
      "weighted avg       0.20      0.20      0.17      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CLSTM (CNN + LSTM) : Combine text and tabular features ---\n",
    "\n",
    "MAX_VOCAB_SIZE_CLSTM = 10000\n",
    "MAX_SEQUENCE_LENGTH_CLSTM = 150 # Should be same as LSTM if using its tokenizer\n",
    "EMBEDDING_DIM_CLSTM = 100     # Should be same as LSTM if using its tokenizer\n",
    "\n",
    "def train_clstm_model(df_input, target_column='concentration', text_tokenizer=None):\n",
    "    \"\"\"\n",
    "    Trains a CLSTM model combining text ('processed_notes') and tabular features\n",
    "    for a classification task.\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): The fully processed DataFrame.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        text_tokenizer (tf.keras.preprocessing.text.Tokenizer, optional): \n",
    "                         A pre-fitted tokenizer. If None, a new one will be fitted.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training CLSTM Model to predict '{target_column}' (Classification) ---\")\n",
    "    df_clstm = df_input.copy()\n",
    "\n",
    "    # 1. Prepare Tabular Features (y_clstm will be adjusted for classification here)\n",
    "    # The prepare_features_for_tabular_model function already handles shifting y and converting to int.\n",
    "    X_tabular_clstm, y_clstm, preprocessor_clstm = prepare_features_for_tabular_model(df_clstm, target_column)\n",
    "    \n",
    "    if X_tabular_clstm is None or y_clstm is None or preprocessor_clstm is None:\n",
    "        print(\"Error: Tabular feature preparation failed for CLSTM. Aborting.\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    # Ensure y_clstm is integer type for classification (should be handled by prepare_features)\n",
    "    if not pd.api.types.is_integer_dtype(y_clstm):\n",
    "         print(f\"Warning: y_clstm for CLSTM is not integer. Type: {y_clstm.dtype}. This should be handled earlier.\")\n",
    "         # Attempt to fix, assuming it was float from -2 to 2\n",
    "         y_clstm = (y_clstm + (2 if y_clstm.min() < 0 else 0)).astype(int)\n",
    "\n",
    "    num_classes_clstm = len(np.unique(y_clstm))\n",
    "    print(f\"  Target '{target_column}' for CLSTM - Unique classes: {np.sort(np.unique(y_clstm))}, Num classes: {num_classes_clstm}\")\n",
    "\n",
    "\n",
    "    # 2. Prepare Text Features\n",
    "    # Ensure 'processed_notes' is present and align with X_tabular_clstm's indices\n",
    "    if 'processed_notes' not in df_clstm.columns:\n",
    "        print(\"Error: 'processed_notes' column missing for CLSTM text features.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    texts_clstm = df_clstm.loc[X_tabular_clstm.index, 'processed_notes'].astype(str).values \n",
    "    \n",
    "    current_tokenizer_clstm = None\n",
    "    if text_tokenizer is None:\n",
    "        print(\"  Fitting new tokenizer for CLSTM text data...\")\n",
    "        current_tokenizer_clstm = Tokenizer(num_words=MAX_VOCAB_SIZE_CLSTM, oov_token=\"<oov>\")\n",
    "        current_tokenizer_clstm.fit_on_texts(texts_clstm)\n",
    "    else:\n",
    "        print(\"  Using provided tokenizer for CLSTM text data...\")\n",
    "        current_tokenizer_clstm = text_tokenizer\n",
    "        \n",
    "    sequences_clstm = current_tokenizer_clstm.texts_to_sequences(texts_clstm)\n",
    "    word_index_clstm = current_tokenizer_clstm.word_index\n",
    "    print(f\"  Found {len(word_index_clstm)} unique tokens for CLSTM (using {'provided' if text_tokenizer else 'new'} tokenizer).\")\n",
    "    \n",
    "    padded_sequences_clstm = pad_sequences(sequences_clstm, maxlen=MAX_SEQUENCE_LENGTH_CLSTM, padding='post', truncating='post')\n",
    "    print(f\"  Shape of padded text sequences for CLSTM: {padded_sequences_clstm.shape}\")\n",
    "\n",
    "    # y_clstm (target) is already prepared from tabular prep, and it's aligned with X_tabular_clstm.\n",
    "    # padded_sequences_clstm is aligned because it uses X_tabular_clstm.index.\n",
    "\n",
    "    # 3. Split Data (Tabular data will be processed after split)\n",
    "    X_train_tab_raw, X_test_tab_raw, \\\n",
    "    X_train_text, X_test_text, \\\n",
    "    y_train_clstm, y_test_clstm = train_test_split(\n",
    "        X_tabular_clstm, padded_sequences_clstm, y_clstm.values, # Use .values for y for consistency\n",
    "        test_size=0.2, random_state=42, stratify=y_clstm.values if num_classes_clstm > 1 else None\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing to tabular data (fit on train, transform test)\n",
    "    print(\"  Preprocessing CLSTM tabular data (fitting on train split)...\")\n",
    "    X_train_tab_processed = preprocessor_clstm.fit_transform(X_train_tab_raw)\n",
    "    X_test_tab_processed = preprocessor_clstm.transform(X_test_tab_raw)\n",
    "    # Keras generally expects dense arrays\n",
    "    if hasattr(X_train_tab_processed, \"toarray\"):\n",
    "        X_train_tab_processed = X_train_tab_processed.toarray()\n",
    "        X_test_tab_processed = X_test_tab_processed.toarray()\n",
    "\n",
    "    print(f\"  CLSTM Train shapes: Tabular {X_train_tab_processed.shape}, Text {X_train_text.shape}, Target {y_train_clstm.shape}\")\n",
    "    print(f\"  CLSTM Test shapes: Tabular {X_test_tab_processed.shape}, Text {X_test_text.shape}, Target {y_test_clstm.shape}\")\n",
    "\n",
    "\n",
    "    # 4. Build CLSTM Model Architecture\n",
    "    print(\"  Building CLSTM model architecture for classification...\")\n",
    "    \n",
    "    # Text Input Branch (Using LSTM as in notebook, not CNN+Pool for this conversion)\n",
    "    input_text = Input(shape=(MAX_SEQUENCE_LENGTH_CLSTM,), name='text_input')\n",
    "    # Ensure vocab size for embedding is correct based on the tokenizer used\n",
    "    embedding_vocab_size = min(MAX_VOCAB_SIZE_CLSTM, len(word_index_clstm) + 1)\n",
    "    embedding_text = Embedding(input_dim=embedding_vocab_size,\n",
    "                               output_dim=EMBEDDING_DIM_CLSTM,\n",
    "                               input_length=MAX_SEQUENCE_LENGTH_CLSTM)(input_text)\n",
    "    # Original notebook had LSTM -> Dense for text features\n",
    "    lstm_text_branch = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embedding_text)\n",
    "    text_features = Dense(32, activation='relu')(lstm_text_branch)\n",
    "\n",
    "    # Tabular Input Branch\n",
    "    input_tabular = Input(shape=(X_train_tab_processed.shape[1],), name='tabular_input')\n",
    "    dense_tabular_branch = Dense(64, activation='relu')(input_tabular) \n",
    "    tabular_features_branch = Dense(32, activation='relu')(dense_tabular_branch)\n",
    "    tabular_features_dropout = Dropout(0.3)(tabular_features_branch)\n",
    "\n",
    "    # Concatenate features\n",
    "    combined_features = concatenate([text_features, tabular_features_dropout])\n",
    "    \n",
    "    # Output layers\n",
    "    combined_dense = Dense(64, activation='relu')(combined_features)\n",
    "    combined_dropout = Dropout(0.4)(combined_dense)\n",
    "    output_clstm = Dense(num_classes_clstm, activation='softmax')(combined_dropout)\n",
    "\n",
    "    model_clstm = Model(inputs=[input_text, input_tabular], outputs=output_clstm)\n",
    "    model_clstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_clstm.summary()\n",
    "\n",
    "    # 5. Train CLSTM Model\n",
    "    print(\"\\n  Training CLSTM model...\")\n",
    "    early_stopping_clstm = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    history_clstm = model_clstm.fit(\n",
    "        [X_train_text, X_train_tab_processed], y_train_clstm, # Ensure correct order of inputs\n",
    "        epochs=40, \n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        # validation_data=([X_test_text, X_test_tab_processed], y_test_clstm), # Alternative: use explicit test set for validation\n",
    "        callbacks=[early_stopping_clstm],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 6. Evaluate CLSTM Model\n",
    "    print(\"\\n  CLSTM Model Evaluation on Test Set (Classification):\")\n",
    "    loss_clstm, accuracy_clstm = model_clstm.evaluate([X_test_text, X_test_tab_processed], y_test_clstm, verbose=0)\n",
    "    print(f\"  Test Loss: {loss_clstm:.4f}\")\n",
    "    print(f\"  Test Accuracy: {accuracy_clstm:.4f}\")\n",
    "\n",
    "    y_pred_probs_clstm = model_clstm.predict([X_test_text, X_test_tab_processed])\n",
    "    y_pred_clstm = np.argmax(y_pred_probs_clstm, axis=1)\n",
    "    \n",
    "    f1_clstm = f1_score(y_test_clstm, y_pred_clstm, average='weighted')\n",
    "    print(f\"  Test F1 Score (weighted): {f1_clstm:.4f}\")\n",
    "    print(\"\\n  Classification Report (CLSTM):\")\n",
    "    print(classification_report(y_test_clstm, y_pred_clstm, zero_division=0))\n",
    "\n",
    "    return model_clstm, current_tokenizer_clstm, preprocessor_clstm, history_clstm\n",
    "\n",
    "\n",
    "# Run CLSTM Model (Predicting concentration using both text and tabular data)\n",
    "# Ensure concentration and processed_notes exist, drop NaNs from these specific columns\n",
    "df_for_clstm_prep = df[['processed_notes', 'concentration'] + df.columns.drop(['processed_notes', 'concentration'], errors='ignore').tolist()].copy()\n",
    "df_for_clstm_prep.dropna(subset=['concentration', 'processed_notes'], inplace=True)\n",
    "\n",
    "# Filter out rows where processed_notes might be empty strings after potential earlier processing, though astype(str) handles most.\n",
    "df_for_clstm_prep = df_for_clstm_prep[df_for_clstm_prep['processed_notes'].str.strip().astype(bool)]\n",
    "\n",
    "\n",
    "if not df_for_clstm_prep.empty:\n",
    "    # Use the tokenizer fitted by the LSTM model if available and if it was for the same text\n",
    "    # Otherwise, train_clstm_model will fit a new one.\n",
    "    # Assuming lstm_tokenizer was trained on processed_notes\n",
    "    clstm_model_trained, clstm_tokenizer_used, clstm_preprocessor_used, clstm_history = train_clstm_model(\n",
    "        df_for_clstm_prep, \n",
    "        target_column='concentration', \n",
    "        text_tokenizer=lstm_tokenizer if 'lstm_tokenizer' in locals() and lstm_tokenizer is not None else None\n",
    "    )\n",
    "else:\n",
    "    print(\"DataFrame for CLSTM is empty after preprocessing. Skipping CLSTM training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b6f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65125831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
