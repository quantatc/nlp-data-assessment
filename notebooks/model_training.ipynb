{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fcfabc",
   "metadata": {},
   "source": [
    "## Model Training – ML & Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15839366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Deep Learning imports (TensorFlow/Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, concatenate, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4fd64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19999 entries, 0 to 19998\n",
      "Data columns (total 24 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   patient_id       19999 non-null  int64  \n",
      " 1   gender           19999 non-null  object \n",
      " 2   medication       19999 non-null  object \n",
      " 3   dose             16651 non-null  object \n",
      " 4   name             19999 non-null  object \n",
      " 5   surname          19999 non-null  object \n",
      " 6   bmi              19999 non-null  float64\n",
      " 7   weight           19999 non-null  float64\n",
      " 8   height           19999 non-null  float64\n",
      " 9   systolic         19999 non-null  int64  \n",
      " 10  diastolic        19999 non-null  int64  \n",
      " 11  concentration    19999 non-null  int64  \n",
      " 12  distractibility  19999 non-null  int64  \n",
      " 13  impulsivity      19999 non-null  int64  \n",
      " 14  hyperactivity    19999 non-null  int64  \n",
      " 15  sleep            19999 non-null  int64  \n",
      " 16  mood             19999 non-null  int64  \n",
      " 17  appetite         19999 non-null  int64  \n",
      " 18  doctor_notes     19999 non-null  object \n",
      " 19  is_medicated     19999 non-null  int64  \n",
      " 20  dose_mg          19999 non-null  float64\n",
      " 21  bmi_category     19999 non-null  object \n",
      " 22  bp_category      19999 non-null  object \n",
      " 23  processed_notes  19999 non-null  object \n",
      "dtypes: float64(4), int64(11), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#load processed data\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/processed/patient_behavior_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58046df6",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81909d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing features for predicting 'concentration'...\n",
      "  Features for tabular model: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg', 'gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified categorical features: ['gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified numerical features: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg']\n"
     ]
    }
   ],
   "source": [
    "# Prepare features: encode categorical and normalise numeric \n",
    "def prepare_features_for_tabular_model(df_input, target_column):\n",
    "    \"\"\"\n",
    "    Prepares tabular features for ML models.\n",
    "    Separates features and target, encodes categorical, scales numerical.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing features for predicting '{target_column}'...\")\n",
    "    df = df_input.copy()\n",
    "\n",
    "    if target_column not in df.columns:\n",
    "        print(f\"Error: Target column '{target_column}' not found in DataFrame.\")\n",
    "        return None, None, None\n",
    "    y = df[target_column] # Define y first\n",
    "\n",
    "    # Define features (X)\n",
    "    # Columns to exclude from features for X_tabular:\n",
    "    # - Identifiers, raw text, original dose string\n",
    "    # - The preprocessed text notes (used by text models)\n",
    "    # - The target column itself\n",
    "    # - Other potential behavioral target columns\n",
    "    behavioral_targets = ['concentration', 'impulsivity', 'mood', 'sleep', 'appetite', 'distractibility', 'hyperactivity']\n",
    "    potential_other_targets = [col for col in behavioral_targets if col != target_column]\n",
    "    \n",
    "    columns_to_drop_for_X = ['patient_id', 'name', 'surname', # Assuming these might exist\n",
    "                                   'doctor_notes', 'dose', \n",
    "                                   'processed_notes', 'processed_notes_nltk', # Exclude all versions of processed notes\n",
    "                                   target_column # CRITICAL: Ensure target column is dropped from features\n",
    "                                  ] + potential_other_targets\n",
    "    \n",
    "    # Ensure only existing columns are dropped\n",
    "    cols_to_drop_existing = [col for col in columns_to_drop_for_X if col in df.columns]\n",
    "    X_tabular = df.drop(columns=cols_to_drop_existing, errors='ignore')\n",
    "    \n",
    "    # Identify categorical and numerical features from the remaining columns in X_tabular\n",
    "    categorical_features = X_tabular.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = X_tabular.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    # This check ensures we only try to process columns that are actually in X_tabular\n",
    "    # Create a copy to avoid SettingWithCopyWarning if X_tabular is a slice\n",
    "    X_tabular = X_tabular[numerical_features + categorical_features].copy() \n",
    "\n",
    "\n",
    "    print(f\"  Features for tabular model: {X_tabular.columns.tolist()}\")\n",
    "    print(f\"  Identified categorical features: {categorical_features}\")\n",
    "    print(f\"  Identified numerical features: {numerical_features}\")\n",
    "    \n",
    "    if X_tabular.empty and not (numerical_features or categorical_features) :\n",
    "        print(\"Error: Feature set X_tabular is empty after dropping columns and identifying types.\")\n",
    "        return None, None, None\n",
    "    elif not numerical_features and not categorical_features and not X_tabular.empty :\n",
    "         print(\"Warning: No specific numerical or categorical features identified by dtype, but X_tabular is not empty. ColumnTransformer might process based on column names if they were manually assigned.\")\n",
    "    \n",
    "    # Create preprocessing pipelines for numerical and categorical features\n",
    "    transformers_list = []\n",
    "    if numerical_features: # Only add transformer if there are numerical features\n",
    "        numerical_transformer = StandardScaler()\n",
    "        transformers_list.append(('num', numerical_transformer, numerical_features))\n",
    "    if categorical_features: # Only add transformer if there are categorical features\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse_output=False for RF\n",
    "        transformers_list.append(('cat', categorical_transformer, categorical_features))\n",
    "\n",
    "    if not transformers_list and not X_tabular.empty: \n",
    "        print(\"Warning: No transformers created (no numerical or categorical features found by dtype to process specifically). Preprocessor will use remainder='passthrough'.\")\n",
    "        preprocessor = ColumnTransformer(transformers=[], remainder='passthrough') \n",
    "    elif not X_tabular.empty:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=transformers_list, \n",
    "            remainder='passthrough' \n",
    "        )\n",
    "    else: # X_tabular is empty, cannot create preprocessor\n",
    "        preprocessor = None\n",
    "        print(\"Error: Cannot create preprocessor as X_tabular is empty.\")\n",
    "\n",
    "    \n",
    "    return X_tabular, y, preprocessor\n",
    "\n",
    "X_tab_rf, y_rf, preprocessor_rf = prepare_features_for_tabular_model(df, target_column='concentration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144a5130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>is_medicated</th>\n",
       "      <th>dose_mg</th>\n",
       "      <th>gender</th>\n",
       "      <th>medication</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>bp_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.8</td>\n",
       "      <td>82.9</td>\n",
       "      <td>1.76</td>\n",
       "      <td>113</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Adderall</td>\n",
       "      <td>Overweight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.3</td>\n",
       "      <td>52.3</td>\n",
       "      <td>1.74</td>\n",
       "      <td>136</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>NoMedication</td>\n",
       "      <td>Underweight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.5</td>\n",
       "      <td>114.9</td>\n",
       "      <td>1.80</td>\n",
       "      <td>128</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Vyvanse</td>\n",
       "      <td>Obese</td>\n",
       "      <td>Elevated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.5</td>\n",
       "      <td>91.2</td>\n",
       "      <td>1.58</td>\n",
       "      <td>101</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>NoMedication</td>\n",
       "      <td>Obese</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.7</td>\n",
       "      <td>40.4</td>\n",
       "      <td>1.51</td>\n",
       "      <td>111</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Vyvanse</td>\n",
       "      <td>Underweight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>23.1</td>\n",
       "      <td>80.8</td>\n",
       "      <td>1.87</td>\n",
       "      <td>131</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Concerta</td>\n",
       "      <td>Normal weight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>30.9</td>\n",
       "      <td>116.2</td>\n",
       "      <td>1.94</td>\n",
       "      <td>120</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Adderall</td>\n",
       "      <td>Obese</td>\n",
       "      <td>Elevated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>11.5</td>\n",
       "      <td>44.8</td>\n",
       "      <td>1.97</td>\n",
       "      <td>91</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Adderall</td>\n",
       "      <td>Underweight</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19.9</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.94</td>\n",
       "      <td>138</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>NoMedication</td>\n",
       "      <td>Normal weight</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>30.4</td>\n",
       "      <td>96.4</td>\n",
       "      <td>1.78</td>\n",
       "      <td>106</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Adderall</td>\n",
       "      <td>Obese</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19999 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bmi  weight  height  systolic  diastolic  is_medicated  dose_mg  \\\n",
       "0      26.8    82.9    1.76       113         88             1     15.0   \n",
       "1      17.3    52.3    1.74       136         72             0      0.0   \n",
       "2      35.5   114.9    1.80       128         77             1     15.0   \n",
       "3      36.5    91.2    1.58       101         78             0      5.0   \n",
       "4      17.7    40.4    1.51       111         81             1     30.0   \n",
       "...     ...     ...     ...       ...        ...           ...      ...   \n",
       "19994  23.1    80.8    1.87       131         77             1     10.0   \n",
       "19995  30.9   116.2    1.94       120         71             1     20.0   \n",
       "19996  11.5    44.8    1.97        91         69             1     30.0   \n",
       "19997  19.9    75.0    1.94       138         89             0      5.0   \n",
       "19998  30.4    96.4    1.78       106         81             1     30.0   \n",
       "\n",
       "       gender    medication   bmi_category           bp_category  \n",
       "0       Other      Adderall     Overweight  Hypertension Stage 1  \n",
       "1       Other  NoMedication    Underweight  Hypertension Stage 1  \n",
       "2        Male       Vyvanse          Obese              Elevated  \n",
       "3      Female  NoMedication          Obese                Normal  \n",
       "4      Female       Vyvanse    Underweight  Hypertension Stage 1  \n",
       "...       ...           ...            ...                   ...  \n",
       "19994   Other      Concerta  Normal weight  Hypertension Stage 1  \n",
       "19995  Female      Adderall          Obese              Elevated  \n",
       "19996    Male      Adderall    Underweight                Normal  \n",
       "19997   Other  NoMedication  Normal weight  Hypertension Stage 1  \n",
       "19998   Other      Adderall          Obese  Hypertension Stage 1  \n",
       "\n",
       "[19999 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aefc1c",
   "metadata": {},
   "source": [
    "### 3.2 2.\tBaseline: Predict `concentration` using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Random Forest Baseline to predict 'concentration' ---\n",
      "Training Random Forest Regressor...\n",
      "\n",
      "Random Forest Regressor Evaluation:\n",
      "  Mean Squared Error (MSE): 2.1203\n",
      "  Mean Absolute Error (MAE): 1.2581\n",
      "  R-squared (R2): -0.0596\n",
      "F1 Score (binary): 0.4238\n",
      "\n",
      "Top 10 Feature Importances (Random Forest):\n",
      "                feature  importance\n",
      "1                weight    0.187710\n",
      "0                   bmi    0.163330\n",
      "3              systolic    0.148484\n",
      "4             diastolic    0.132025\n",
      "2                height    0.125509\n",
      "6               dose_mg    0.068058\n",
      "13   medication_Ritalin    0.016243\n",
      "8           gender_Male    0.015886\n",
      "15   medication_Vyvanse    0.015445\n",
      "11  medication_Concerta    0.015329\n"
     ]
    }
   ],
   "source": [
    "def train_random_forest_baseline(X_tabular_input, y_target_input, preprocessor_input):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest Regressor to predict 'concentration'.\n",
    "    \"\"\"\n",
    "    target_name = y_target_input.name\n",
    "    print(f\"\\n--- Training Random Forest Baseline to predict '{target_name}' ---\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tabular_input, y_target_input, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create the full pipeline including preprocessing and the model\n",
    "    # RandomForestRegressor as concentration is a numerical rating (-2 to 2)\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "                                        # n_estimators=500,  # Increased number of trees\n",
    "                                        # max_depth=15,      # Allowing deeper trees\n",
    "                                        # min_samples_split=3,  # More aggressive splitting\n",
    "                                        # min_samples_leaf=1,   # More granular leaf nodes\n",
    "                                        # max_features='sqrt',  # Standard RF feature selection\n",
    "                                        # random_state=42,\n",
    "                                        # n_jobs=-1  # Use all available cores\n",
    "                                        # )\n",
    "    \n",
    "    pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor_input),\n",
    "                                  ('regressor', rf_model)])\n",
    "\n",
    "    print(\"Training Random Forest Regressor...\")\n",
    "    pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_rf = pipeline_rf.predict(X_test)\n",
    "    \n",
    "    print(\"\\nRandom Forest Regressor Evaluation:\")\n",
    "    mse = mean_squared_error(y_test, y_pred_rf)\n",
    "    mae = mean_absolute_error(y_test, y_pred_rf) # MAE is required by assessment\n",
    "    r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "    # For F1 score, we need to convert regression to classification\n",
    "    # Let's create binary classes based on median value for demonstration\n",
    "    y_test_binary = (y_test > y_test.median()).astype(int)\n",
    "    y_pred_binary = (y_pred_rf> y_test.median()).astype(int)\n",
    "    f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "    print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\") # Key metric\n",
    "    print(f\"  R-squared (R2): {r2:.4f}\")\n",
    "    print(f\"F1 Score (binary): {f1:.4f}\")\n",
    "\n",
    "    # Feature importances\n",
    "    try:\n",
    "        # Get feature names after one-hot encoding\n",
    "        ohe_feature_names = pipeline_rf.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(\n",
    "            X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        )\n",
    "        num_feature_names = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "        all_feature_names = np.concatenate([num_feature_names, ohe_feature_names])\n",
    "        \n",
    "        importances = pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "        print(\"\\nTop 10 Feature Importances (Random Forest):\")\n",
    "        print(feature_importance_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve feature importances: {e}\")\n",
    "        \n",
    "    return pipeline_rf # Return the trained pipeline\n",
    "\n",
    "rf_pipeline_trained = train_random_forest_baseline(X_tab_rf, y_rf, preprocessor_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732cddc9",
   "metadata": {},
   "source": [
    "### 3.3.1\tDeep models: Predict `impulsivity` from doctor_notes using LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LSTM Model to predict 'impulsivity' ---\n",
      "  Found 822 unique tokens.\n",
      "  Shape of padded sequences: (19999, 150)\n",
      "  Building LSTM model architecture...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 150, 100)          82300     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126,653\n",
      "Trainable params: 126,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "  Training LSTM model...\n",
      "Epoch 1/30\n",
      "450/450 [==============================] - 160s 343ms/step - loss: 2.0052 - mean_absolute_error: 1.2132 - val_loss: 2.0335 - val_mean_absolute_error: 1.2128\n",
      "Epoch 2/30\n",
      "450/450 [==============================] - 154s 343ms/step - loss: 2.0040 - mean_absolute_error: 1.2090 - val_loss: 2.0321 - val_mean_absolute_error: 1.2150\n",
      "Epoch 3/30\n",
      "450/450 [==============================] - 150s 333ms/step - loss: 2.0041 - mean_absolute_error: 1.2084 - val_loss: 2.0308 - val_mean_absolute_error: 1.2179\n",
      "Epoch 4/30\n",
      "450/450 [==============================] - 145s 322ms/step - loss: 2.0039 - mean_absolute_error: 1.2066 - val_loss: 2.0313 - val_mean_absolute_error: 1.2165\n",
      "Epoch 5/30\n",
      "450/450 [==============================] - 154s 343ms/step - loss: 2.0038 - mean_absolute_error: 1.2077 - val_loss: 2.0318 - val_mean_absolute_error: 1.2156\n",
      "Epoch 6/30\n",
      "450/450 [==============================] - 154s 342ms/step - loss: 2.0038 - mean_absolute_error: 1.2067 - val_loss: 2.0316 - val_mean_absolute_error: 1.2159\n",
      "Epoch 7/30\n",
      "450/450 [==============================] - 152s 338ms/step - loss: 2.0038 - mean_absolute_error: 1.2069 - val_loss: 2.0314 - val_mean_absolute_error: 1.2165\n",
      "Epoch 8/30\n",
      "450/450 [==============================] - 155s 345ms/step - loss: 2.0038 - mean_absolute_error: 1.2077 - val_loss: 2.0318 - val_mean_absolute_error: 1.2155\n",
      "\n",
      "  LSTM Model Evaluation on Test Set:\n",
      "  Test Loss (MSE): 2.0418\n",
      "  Test Mean Absolute Error (MAE): 1.2245\n",
      "125/125 [==============================] - 10s 72ms/step\n",
      "  Test R-squared (R2): -0.0005\n"
     ]
    }
   ],
   "source": [
    "# --- Deep models: LSTM : Predict impulsivity from doctor_notes ---\n",
    "MAX_VOCAB_SIZE_LSTM = 10000\n",
    "MAX_SEQUENCE_LENGTH_LSTM = 150 # Max length of sequences (notes)\n",
    "EMBEDDING_DIM_LSTM = 100 # Dimension of word embeddings\n",
    "\n",
    "def train_lstm_model(df_input, target_column='impulsivity'):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model to predict a target (e.g., 'impulsivity') from 'processed_notes'.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training LSTM Model to predict '{target_column}' ---\")\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    texts = df['processed_notes'].values\n",
    "    labels = df[target_column].values\n",
    "\n",
    "    # Tokenizer: Convert text to sequences of integers\n",
    "    tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE_LSTM, oov_token=\"<oov>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(f\"  Found {len(word_index)} unique tokens.\")\n",
    "\n",
    "    # Pad sequences to ensure uniform length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH_LSTM, padding='post', truncating='post')\n",
    "    print(f\"  Shape of padded sequences: {padded_sequences.shape}\")\n",
    "\n",
    "    # Prepare labels for regression (LSTM can output continuous values)\n",
    "    # If labels are categorical, you'd one-hot encode or use sparse_categorical_crossentropy\n",
    "    # For regression target like -2 to 2, no change needed for y, but ensure it's float\n",
    "    y_lstm = labels.astype(float)\n",
    "\n",
    "    # Split data\n",
    "    X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "        padded_sequences, y_lstm, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Build LSTM model\n",
    "    print(\"  Building LSTM model architecture...\")\n",
    "    input_lstm = Input(shape=(MAX_SEQUENCE_LENGTH_LSTM,))\n",
    "    embedding_layer = Embedding(input_dim=min(MAX_VOCAB_SIZE_LSTM, len(word_index) + 1), # Vocab size +1 for padding\n",
    "                                output_dim=EMBEDDING_DIM_LSTM, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH_LSTM)(input_lstm)\n",
    "    lstm_layer = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embedding_layer) # 64 units\n",
    "    dense_layer = Dense(32, activation='relu')(lstm_layer)\n",
    "    dropout_layer = Dropout(0.3)(dense_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dropout_layer) # Linear activation for regression\n",
    "\n",
    "    model_lstm = Model(inputs=input_lstm, outputs=output_layer)\n",
    "    model_lstm.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    model_lstm.summary()\n",
    "\n",
    "    print(\"\\n  Training LSTM model...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history_lstm = model_lstm.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=30, # Adjust epochs\n",
    "        batch_size=32,\n",
    "        validation_split=0.1, # Use part of training data for validation during training\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate LSTM model\n",
    "    print(\"\\n  LSTM Model Evaluation on Test Set:\")\n",
    "    loss_lstm, mae_lstm = model_lstm.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "    print(f\"  Test Loss (MSE): {loss_lstm:.4f}\")\n",
    "    print(f\"  Test Mean Absolute Error (MAE): {mae_lstm:.4f}\") # Key metric\n",
    "\n",
    "    # To get R2, we need predictions\n",
    "    y_pred_lstm_test = model_lstm.predict(X_test_lstm).flatten()\n",
    "    r2_lstm = r2_score(y_test_lstm, y_pred_lstm_test)\n",
    "    print(f\"  Test R-squared (R2): {r2_lstm:.4f}\")\n",
    "    \n",
    "    return model_lstm, tokenizer, history_lstm # Return tokenizer for CLSTM if needed\n",
    "\n",
    "# Run LSTM Model (Predicting impulsivity from doctor_notes)\n",
    "df_for_lstm = df[['processed_notes', 'impulsivity']].copy().dropna()\n",
    "lstm_model_trained, lstm_tokenizer, _ = train_lstm_model(df_for_lstm, target_column='impulsivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e130978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634dc5bd",
   "metadata": {},
   "source": [
    "### 3.3.2\tDeep models: Predict `concentration` from doctor_notes using CLSTM (CNN + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d46f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training CLSTM Model to predict 'concentration' ---\n",
      "\n",
      "Preparing features for predicting 'concentration'...\n",
      "  Features for tabular model: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg', 'gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified categorical features: ['gender', 'medication', 'bmi_category', 'bp_category']\n",
      "  Identified numerical features: ['bmi', 'weight', 'height', 'systolic', 'diastolic', 'is_medicated', 'dose_mg']\n",
      "  Shape of processed tabular data: (19999, 24)\n",
      "  Using provided tokenizer for CLSTM text data...\n",
      "  Found 822 unique tokens for CLSTM.\n",
      "  Shape of padded text sequences for CLSTM: (19999, 150)\n",
      "  CLSTM Train shapes: Tabular (15999, 24), Text (15999, 150), Target (15999,)\n",
      "  CLSTM Test shapes: Tabular (4000, 24), Text (4000, 150), Target (4000,)\n",
      "  Building CLSTM model architecture...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " tabular_input (InputLayer)     [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 150, 100)     82300       ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           1600        ['tabular_input[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 64)           42240       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           2080        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['dense_2[0][0]',                \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            65          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134,525\n",
      "Trainable params: 134,525\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "  Training CLSTM model...\n",
      "Epoch 1/40\n",
      "450/450 [==============================] - 167s 360ms/step - loss: 2.0242 - mean_absolute_error: 1.2215 - val_loss: 1.9481 - val_mean_absolute_error: 1.1789\n",
      "Epoch 2/40\n",
      "450/450 [==============================] - 142s 316ms/step - loss: 2.0128 - mean_absolute_error: 1.2143 - val_loss: 1.9467 - val_mean_absolute_error: 1.1788\n",
      "Epoch 3/40\n",
      "450/450 [==============================] - 142s 316ms/step - loss: 2.0115 - mean_absolute_error: 1.2122 - val_loss: 1.9507 - val_mean_absolute_error: 1.1783\n",
      "Epoch 4/40\n",
      "450/450 [==============================] - 143s 318ms/step - loss: 2.0104 - mean_absolute_error: 1.2135 - val_loss: 1.9506 - val_mean_absolute_error: 1.1795\n",
      "Epoch 5/40\n",
      "450/450 [==============================] - 144s 321ms/step - loss: 2.0106 - mean_absolute_error: 1.2134 - val_loss: 1.9464 - val_mean_absolute_error: 1.1777\n",
      "Epoch 6/40\n",
      "450/450 [==============================] - 154s 342ms/step - loss: 2.0050 - mean_absolute_error: 1.2129 - val_loss: 1.9494 - val_mean_absolute_error: 1.1795\n",
      "Epoch 7/40\n",
      "450/450 [==============================] - 155s 344ms/step - loss: 2.0049 - mean_absolute_error: 1.2131 - val_loss: 1.9540 - val_mean_absolute_error: 1.1826\n",
      "Epoch 8/40\n",
      "450/450 [==============================] - 154s 342ms/step - loss: 2.0033 - mean_absolute_error: 1.2142 - val_loss: 1.9534 - val_mean_absolute_error: 1.1821\n",
      "Epoch 9/40\n",
      "450/450 [==============================] - 154s 342ms/step - loss: 2.0015 - mean_absolute_error: 1.2131 - val_loss: 1.9543 - val_mean_absolute_error: 1.1826\n",
      "Epoch 10/40\n",
      "450/450 [==============================] - 167s 372ms/step - loss: 2.0008 - mean_absolute_error: 1.2128 - val_loss: 1.9523 - val_mean_absolute_error: 1.1806\n",
      "Epoch 11/40\n",
      "450/450 [==============================] - 170s 377ms/step - loss: 1.9956 - mean_absolute_error: 1.2120 - val_loss: 1.9552 - val_mean_absolute_error: 1.1832\n",
      "Epoch 12/40\n",
      "450/450 [==============================] - 161s 358ms/step - loss: 1.9936 - mean_absolute_error: 1.2133 - val_loss: 1.9590 - val_mean_absolute_error: 1.1863\n",
      "\n",
      "  CLSTM Model Evaluation on Test Set:\n",
      "  Test Loss (MSE): 2.0070\n",
      "  Test Mean Absolute Error (MAE): 1.2075\n",
      "125/125 [==============================] - 17s 125ms/step\n",
      "  Test R-squared (R2): -0.0029\n"
     ]
    }
   ],
   "source": [
    "# --- Section 3.4: CLSTM (CNN + LSTM) : Combine text and tabular features ---\n",
    "# This is more complex. Let's define a function stub and outline steps.\n",
    "# We need to decide on the target for CLSTM. Let's assume 'concentration' for comparison.\n",
    "\n",
    "MAX_VOCAB_SIZE_CLSTM = 10000\n",
    "MAX_SEQUENCE_LENGTH_CLSTM = 150\n",
    "EMBEDDING_DIM_CLSTM = 100\n",
    "\n",
    "def train_clstm_model(df_input, target_column='concentration', text_tokenizer=None):\n",
    "    \"\"\"\n",
    "    Trains a CLSTM model combining text ('processed_notes') and tabular features.\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): The fully processed DataFrame.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        text_tokenizer (tf.keras.preprocessing.text.Tokenizer, optional): \n",
    "                         A pre-fitted tokenizer. If None, a new one will be fitted.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training CLSTM Model to predict '{target_column}' ---\")\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # Prepare Tabular Features\n",
    "    X_tabular_clstm, y_clstm, preprocessor_clstm = prepare_features_for_tabular_model(df, target_column)\n",
    "    \n",
    "    # Apply preprocessing to the entire tabular dataset\n",
    "    X_tabular_processed = preprocessor_clstm.fit_transform(X_tabular_clstm)\n",
    "    print(f\"  Shape of processed tabular data: {X_tabular_processed.shape}\")\n",
    "\n",
    "    # Prepare Text Features\n",
    "    texts_clstm = df.loc[X_tabular_clstm.index, 'processed_notes'].values # Ensure alignment with tabular data\n",
    "    \n",
    "    if text_tokenizer is None:\n",
    "        print(\"  Fitting new tokenizer for CLSTM text data...\")\n",
    "        tokenizer_clstm = Tokenizer(num_words=MAX_VOCAB_SIZE_CLSTM, oov_token=\"<oov>\")\n",
    "        tokenizer_clstm.fit_on_texts(texts_clstm)\n",
    "    else:\n",
    "        print(\"  Using provided tokenizer for CLSTM text data...\")\n",
    "        tokenizer_clstm = text_tokenizer\n",
    "        \n",
    "    sequences_clstm = tokenizer_clstm.texts_to_sequences(texts_clstm)\n",
    "    word_index_clstm = tokenizer_clstm.word_index\n",
    "    print(f\"  Found {len(word_index_clstm)} unique tokens for CLSTM.\")\n",
    "    \n",
    "    padded_sequences_clstm = pad_sequences(sequences_clstm, maxlen=MAX_SEQUENCE_LENGTH_CLSTM, padding='post', truncating='post')\n",
    "    print(f\"  Shape of padded text sequences for CLSTM: {padded_sequences_clstm.shape}\")\n",
    "\n",
    "    # Ensure y_clstm is float for regression\n",
    "    y_clstm = y_clstm.astype(float).values\n",
    "\n",
    "    # Split Data (for both Tabular and Text)\n",
    "    X_train_tab, X_test_tab, \\\n",
    "    X_train_text, X_test_text, \\\n",
    "    y_train_clstm, y_test_clstm = train_test_split(\n",
    "        X_tabular_processed, padded_sequences_clstm, y_clstm,\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"  CLSTM Train shapes: Tabular {X_train_tab.shape}, Text {X_train_text.shape}, Target {y_train_clstm.shape}\")\n",
    "    print(f\"  CLSTM Test shapes: Tabular {X_test_tab.shape}, Text {X_test_text.shape}, Target {y_test_clstm.shape}\")\n",
    "\n",
    "    # Build CLSTM Model Architecture\n",
    "    print(\"  Building CLSTM model architecture...\")\n",
    "    \n",
    "    # Text Input Branch (CNN + LSTM)\n",
    "    input_text = Input(shape=(MAX_SEQUENCE_LENGTH_CLSTM,), name='text_input')\n",
    "    embedding_text = Embedding(input_dim=min(MAX_VOCAB_SIZE_CLSTM, len(word_index_clstm) + 1),\n",
    "                               output_dim=EMBEDDING_DIM_CLSTM,\n",
    "                               input_length=MAX_SEQUENCE_LENGTH_CLSTM)(input_text)\n",
    "    conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding_text)\n",
    "    pool_layer = GlobalMaxPooling1D()(conv_layer) # Or LSTM after Conv1D\n",
    "    \n",
    "    # Text Input Branch (LSTM)\n",
    "    # input_text = Input(shape=(MAX_SEQUENCE_LENGTH_CLSTM,), name='text_input')\n",
    "    # embedding_text = Embedding(...)(input_text)\n",
    "    lstm_text = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embedding_text)\n",
    "    text_features = Dense(32, activation='relu')(lstm_text)\n",
    "\n",
    "    # Tabular Input Branch\n",
    "    input_tabular = Input(shape=(X_train_tab.shape[1],), name='tabular_input')\n",
    "    dense_tabular = Dense(64, activation='relu')(input_tabular)\n",
    "    tabular_features = Dense(32, activation='relu')(dense_tabular)\n",
    "    tabular_features = Dropout(0.3)(tabular_features)\n",
    "\n",
    "    # Concatenate features\n",
    "    combined_features = concatenate([text_features, tabular_features])\n",
    "    \n",
    "    # Output layers\n",
    "    combined_dense = Dense(64, activation='relu')(combined_features)\n",
    "    combined_dropout = Dropout(0.4)(combined_dense)\n",
    "    output_clstm = Dense(1, activation='linear')(combined_dropout) # Regression\n",
    "\n",
    "    model_clstm = Model(inputs=[input_text, input_tabular], outputs=output_clstm)\n",
    "    model_clstm.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    model_clstm.summary()\n",
    "\n",
    "    # Train CLSTM Model\n",
    "    print(\"\\n  Training CLSTM model...\")\n",
    "    early_stopping_clstm = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    history_clstm = model_clstm.fit(\n",
    "        [X_train_text, X_train_tab], y_train_clstm,\n",
    "        epochs=40, # Adjust epochs\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping_clstm],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate CLSTM Model\n",
    "    print(\"\\n  CLSTM Model Evaluation on Test Set:\")\n",
    "    loss_clstm, mae_clstm = model_clstm.evaluate([X_test_text, X_test_tab], y_test_clstm, verbose=0)\n",
    "    print(f\"  Test Loss (MSE): {loss_clstm:.4f}\")\n",
    "    print(f\"  Test Mean Absolute Error (MAE): {mae_clstm:.4f}\")\n",
    "\n",
    "    y_pred_clstm_test = model_clstm.predict([X_test_text, X_test_tab]).flatten()\n",
    "    r2_clstm = r2_score(y_test_clstm, y_pred_clstm_test)\n",
    "    print(f\"  Test R-squared (R2): {r2_clstm:.4f}\")\n",
    "\n",
    "    return model_clstm, tokenizer_clstm, preprocessor_clstm, history_clstm\n",
    "\n",
    "\n",
    "# Run CLSTM Model (Predicting concentration using both text and tabular data)\n",
    "df_for_clstm = df.copy().dropna(subset=['concentration', 'processed_notes'])\n",
    "df_for_clstm = df_for_clstm[df_for_clstm['processed_notes'].str.strip().astype(bool)]\n",
    "\n",
    "clstm_model_trained, _, _, _ = train_clstm_model(df_for_clstm, target_column='concentration', text_tokenizer=lstm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b6f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65125831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
